{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273bf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d67aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN file fold_0.csv] Checking and fixing allele formats...\n",
      "[TRAIN file fold_0.csv] No mismatch in allele found.\n",
      "[TRAIN file fold_1.csv] Checking and fixing allele formats...\n",
      "[TRAIN file fold_1.csv] No mismatch in allele found.\n",
      "[TRAIN file fold_2.csv] Checking and fixing allele formats...\n",
      "[TRAIN file fold_2.csv] No mismatch in allele found.\n",
      "[TRAIN file fold_3.csv] Checking and fixing allele formats...\n",
      "[TRAIN file fold_3.csv] No mismatch in allele found.\n",
      "[TRAIN file fold_4.csv] Checking and fixing allele formats...\n",
      "[TRAIN file fold_4.csv] No mismatch in allele found.\n",
      "[TEST] Checking and fixing allele formats...\n",
      "[DEBUG] Correcting allele naming...\n",
      "[TEST] All allele names corrected successfully.\n",
      "\n",
      "=== Dataset Stats: TRAIN (raw) ===\n",
      "Rows: 3,679,405\n",
      "Columns: ['peptide', 'allele', 'hit']\n",
      "Unique alleles: 130\n",
      "Label distribution (hit):\n",
      "hit\n",
      "0    3481858\n",
      "1     197547\n",
      "Name: count, dtype: int64\n",
      "Peptide length stats:\n",
      "count    3.679405e+06\n",
      "mean     1.080296e+01\n",
      "std      1.964097e+00\n",
      "min      8.000000e+00\n",
      "25%      9.000000e+00\n",
      "50%      1.100000e+01\n",
      "75%      1.200000e+01\n",
      "max      1.500000e+01\n",
      "Name: peptide, dtype: float64\n",
      "\n",
      "=== Dataset Stats: TEST (raw) ===\n",
      "Rows: 946,141\n",
      "Columns: ['peptide', 'hit', 'allele']\n",
      "Unique alleles: 36\n",
      "Label distribution (hit):\n",
      "hit\n",
      "0    900725\n",
      "1     45416\n",
      "Name: count, dtype: int64\n",
      "Peptide length stats:\n",
      "count    946141.000000\n",
      "mean         10.931184\n",
      "std           1.992527\n",
      "min           8.000000\n",
      "25%           9.000000\n",
      "50%          11.000000\n",
      "75%          13.000000\n",
      "max          14.000000\n",
      "Name: peptide, dtype: float64\n",
      "[TRAIN] Removed exact duplicate rows: 0\n",
      "[TEST] Removed exact duplicate rows: 126\n",
      "[TRAIN] Conflicting duplicates: none.\n",
      "[TEST] Conflicting (peptide, allele) pairs: 7 | Rows removed: 14\n",
      "[TRAIN] Missing values: none.\n",
      "[TEST] Missing values: none.\n",
      "[TRAIN] Non-standard/invalid peptide rows removed: 4\n",
      "[TRAIN] Sequence length stats BEFORE removal:\n",
      "count    3.679405e+06\n",
      "mean     1.080296e+01\n",
      "std      1.964097e+00\n",
      "min      8.000000e+00\n",
      "25%      9.000000e+00\n",
      "50%      1.100000e+01\n",
      "75%      1.200000e+01\n",
      "max      1.500000e+01\n",
      "Name: peptide, dtype: float64\n",
      "[TRAIN] Sequence length stats AFTER removal:\n",
      "count    3.679401e+06\n",
      "mean     1.080296e+01\n",
      "std      1.964098e+00\n",
      "min      8.000000e+00\n",
      "25%      9.000000e+00\n",
      "50%      1.100000e+01\n",
      "75%      1.200000e+01\n",
      "max      1.500000e+01\n",
      "Name: peptide, dtype: float64\n",
      "[TEST] Non-standard/invalid peptide rows removed: 0\n",
      "[TEST] Sequence length stats BEFORE removal:\n",
      "count    946001.000000\n",
      "mean         10.931258\n",
      "std           1.992526\n",
      "min           8.000000\n",
      "25%           9.000000\n",
      "50%          11.000000\n",
      "75%          13.000000\n",
      "max          14.000000\n",
      "Name: peptide, dtype: float64\n",
      "[TEST] Sequence length stats AFTER removal:\n",
      "count    946001.000000\n",
      "mean         10.931258\n",
      "std           1.992526\n",
      "min           8.000000\n",
      "25%           9.000000\n",
      "50%          11.000000\n",
      "75%          13.000000\n",
      "max          14.000000\n",
      "Name: peptide, dtype: float64\n",
      "[TEST] Alleles not present in TRAIN: ['HLA-A*02:02', 'HLA-A*02:11', 'HLA-A*33:01', 'HLA-B*53:01']\n",
      "[TEST] Removed rows with unknown alleles: 215768\n",
      "\n",
      "=== Dataset Stats: TRAIN (cleaned) ===\n",
      "Rows: 3,679,401\n",
      "Columns: ['peptide', 'allele', 'hit']\n",
      "Unique alleles: 130\n",
      "Label distribution (hit):\n",
      "hit\n",
      "0    3481858\n",
      "1     197543\n",
      "Name: count, dtype: int64\n",
      "Peptide length stats:\n",
      "count    3.679401e+06\n",
      "mean     1.080296e+01\n",
      "std      1.964098e+00\n",
      "min      8.000000e+00\n",
      "25%      9.000000e+00\n",
      "50%      1.100000e+01\n",
      "75%      1.200000e+01\n",
      "max      1.500000e+01\n",
      "Name: peptide, dtype: float64\n",
      "\n",
      "=== Dataset Stats: TEST (cleaned) ===\n",
      "Rows: 730,233\n",
      "Columns: ['peptide', 'hit', 'allele']\n",
      "Unique alleles: 32\n",
      "Label distribution (hit):\n",
      "hit\n",
      "0    694073\n",
      "1     36160\n",
      "Name: count, dtype: int64\n",
      "Peptide length stats:\n",
      "count    730233.000000\n",
      "mean         10.929087\n",
      "std           1.993217\n",
      "min           8.000000\n",
      "25%           9.000000\n",
      "50%          11.000000\n",
      "75%          13.000000\n",
      "max          14.000000\n",
      "Name: peptide, dtype: float64\n",
      "\n",
      "[INFO] Generating distribution plots...\n",
      "[INFO] Figures saved to clean\n",
      "[FE] Max sequence length (from CLEANED TRAIN): 15\n",
      "\n",
      "Saved cleaned TRAIN to: clean\\train_clean.csv\n",
      "Saved cleaned TEST  to: clean\\test_clean.csv\n",
      "Saved metadata to: clean\\metadata.json\n",
      "Saved figures to: clean\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_DIR = \"datasets\"\n",
    "TRAIN_FOLDS = [f\"fold_{i}.csv\" for i in range(5)]\n",
    "TEST_FILE = \"test.csv\"\n",
    "\n",
    "TRAIN_PATHS = [os.path.join(DATA_DIR, f) for f in TRAIN_FOLDS]\n",
    "TEST_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "\n",
    "# Output directory\n",
    "CLEAN_DIR = \"clean\"\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "# Valid amino acids (standard 20)\n",
    "AA_VOCAB = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_SET = set(AA_VOCAB)\n",
    "\n",
    "# -----------------------\n",
    "# Allele name correction utilities\n",
    "# -----------------------\n",
    "def fix_allele_format(allele: str) -> str:\n",
    "    \"\"\"Fix allele name to follow standard HLA format: HLA-A*02:101\"\"\"\n",
    "    allele = str(allele).strip().upper()\n",
    "    valid_pattern = re.compile(r\"^HLA-[A-Z]\\*\\d{2}:\\d{2,3}$\")\n",
    "\n",
    "    if valid_pattern.match(allele):\n",
    "        return allele  # already valid\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    allele = allele.replace(\" \", \"\").replace(\"_\", \"\").replace(\"--\", \"-\")\n",
    "\n",
    "    # Ensure starts with HLA-\n",
    "    if not allele.startswith(\"HLA\"):\n",
    "        allele = \"HLA-\" + allele\n",
    "    elif not allele.startswith(\"HLA-\"):\n",
    "        allele = allele.replace(\"HLA\", \"HLA-\", 1)\n",
    "\n",
    "    # Insert \"*\" if missing\n",
    "    if \"*\" not in allele:\n",
    "        allele = re.sub(r\"^(HLA-[A-Z])(\\d+)\", r\"\\1*\\2\", allele)\n",
    "\n",
    "    # Replace wrong separators (e.g., HLA-A*0201 -> HLA-A*02:01)\n",
    "    allele = re.sub(r\"(\\*\\d{2})(\\d{2,3})$\", r\"\\1:\\2\", allele)\n",
    "\n",
    "    return allele\n",
    "\n",
    "\n",
    "def clean_allele_column(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Validate and fix allele names in a DataFrame\"\"\"\n",
    "    print(f\"[{name}] Checking and fixing allele formats...\")\n",
    "\n",
    "    valid_pattern = re.compile(r\"^HLA-[A-Z]\\*\\d{2}:\\d{2,3}$\")\n",
    "\n",
    "    corrections_made = False\n",
    "    corrected_alleles = []\n",
    "\n",
    "    for allele in df[\"allele\"]:\n",
    "        corrected = fix_allele_format(allele)\n",
    "        if corrected != allele:\n",
    "            corrections_made = True\n",
    "        corrected_alleles.append(corrected)\n",
    "\n",
    "    if corrections_made:\n",
    "        print(f\"[DEBUG] Correcting allele naming...\")\n",
    "\n",
    "    df[\"allele\"] = corrected_alleles\n",
    "\n",
    "    invalid_mask = ~df[\"allele\"].str.match(valid_pattern)\n",
    "    invalid_count = invalid_mask.sum()\n",
    "\n",
    "    if invalid_count > 0:\n",
    "        print(f\"[{name}] WARNING: {invalid_count} allele entries still invalid after correction.\")\n",
    "    elif corrections_made:\n",
    "        print(f\"[{name}] All allele names corrected successfully.\")\n",
    "    else:\n",
    "        print(f\"[{name}] No mismatch in allele found.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Utils\n",
    "# -----------------------\n",
    "def load_csv_safe(path: str) -> pd.DataFrame:\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    expected_cols = {\"peptide\", \"allele\", \"hit\"}\n",
    "    missing = expected_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path} missing columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "def dataset_stats(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== Dataset Stats: {name} ===\")\n",
    "    print(f\"Rows: {len(df):,}\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Unique alleles:\", df[\"allele\"].nunique())\n",
    "    if \"hit\" in df.columns:\n",
    "        print(\"Label distribution (hit):\")\n",
    "        print(df[\"hit\"].value_counts(dropna=False))\n",
    "    lens = df[\"peptide\"].astype(str).str.len()\n",
    "    print(\"Peptide length stats:\")\n",
    "    print(lens.describe())\n",
    "\n",
    "def remove_exact_duplicates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    before = len(df)\n",
    "    df2 = df.drop_duplicates()\n",
    "    print(f\"[{name}] Removed exact duplicate rows: {before - len(df2)}\")\n",
    "    return df2\n",
    "\n",
    "def remove_conflicting_duplicates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    grp = df.groupby([\"peptide\", \"allele\"])[\"hit\"].nunique()\n",
    "    conflicts = grp[grp > 1]\n",
    "    if conflicts.empty:\n",
    "        print(f\"[{name}] Conflicting duplicates: none.\")\n",
    "        return df\n",
    "    before = len(df)\n",
    "    bad_keys = set(conflicts.index)\n",
    "    mask = df.set_index([\"peptide\", \"allele\"]).index.isin(bad_keys)\n",
    "    df2 = df[~mask].copy()\n",
    "    print(f\"[{name}] Conflicting (peptide, allele) pairs: {len(conflicts)} | Rows removed: {before - len(df2)}\")\n",
    "    return df2\n",
    "\n",
    "def handle_missing(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    miss = df.isna().sum()\n",
    "    if miss.sum() == 0:\n",
    "        print(f\"[{name}] Missing values: none.\")\n",
    "        return df\n",
    "    print(f\"[{name}] Missing values per column:\\n{miss}\")\n",
    "    before = len(df)\n",
    "    df2 = df.dropna(subset=[\"peptide\", \"allele\", \"hit\"]).copy()\n",
    "    print(f\"[{name}] Rows dropped due to missing peptide/allele/hit: {before - len(df2)}\")\n",
    "    return df2\n",
    "\n",
    "def is_valid_peptide(seq: str) -> bool:\n",
    "    s = str(seq).strip().upper()\n",
    "    if len(s) == 0:\n",
    "        return False\n",
    "    return set(s).issubset(AA_SET)\n",
    "\n",
    "def clean_invalid_peptides(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    lens_before = df[\"peptide\"].astype(str).str.len().describe()\n",
    "    valid_mask = df[\"peptide\"].astype(str).str.upper().apply(is_valid_peptide)\n",
    "    invalid = (~valid_mask).sum()\n",
    "    df2 = df[valid_mask].copy()\n",
    "    print(f\"[{name}] Non-standard/invalid peptide rows removed: {invalid}\")\n",
    "    print(f\"[{name}] Sequence length stats BEFORE removal:\\n{lens_before}\")\n",
    "    print(f\"[{name}] Sequence length stats AFTER removal:\\n{df2['peptide'].astype(str).str.len().describe()}\")\n",
    "    return df2\n",
    "\n",
    "def filter_test_alleles_in_train(train: pd.DataFrame, test: pd.DataFrame) -> pd.DataFrame:\n",
    "    train_alleles = set(train[\"allele\"].unique())\n",
    "    test_alleles  = set(test[\"allele\"].unique())\n",
    "    unknown = sorted(list(test_alleles - train_alleles))\n",
    "    if unknown:\n",
    "        before = len(test)\n",
    "        test2 = test[test[\"allele\"].isin(train_alleles)].copy()\n",
    "        print(f\"[TEST] Alleles not present in TRAIN: {unknown}\")\n",
    "        print(f\"[TEST] Removed rows with unknown alleles: {before - len(test2)}\")\n",
    "        return test2\n",
    "    print(\"[TEST] All test alleles exist in training.\")\n",
    "    return test\n",
    "\n",
    "def compute_max_len(train_df: pd.DataFrame) -> int:\n",
    "    max_len = int(train_df[\"peptide\"].astype(str).str.len().max())\n",
    "    print(f\"[FE] Max sequence length (from CLEANED TRAIN): {max_len}\")\n",
    "    return max_len\n",
    "\n",
    "# -----------------------\n",
    "# Visualization helpers\n",
    "# -----------------------\n",
    "def plot_class_distribution(df, out_prefix, top_n_alleles=15):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.countplot(x=\"hit\", data=df)\n",
    "    plt.title(\"General Class Distribution (hit)\")\n",
    "    plt.xlabel(\"Class (hit)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_class_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    allele_counts = (\n",
    "        df.groupby([\"allele\",\"hit\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    top_alleles = df[\"allele\"].value_counts().head(top_n_alleles).index\n",
    "    sns.barplot(\n",
    "        x=\"allele\", y=\"count\", hue=\"hit\",\n",
    "        data=allele_counts[allele_counts[\"allele\"].isin(top_alleles)]\n",
    "    )\n",
    "    plt.title(f\"Class Distribution per Allele (top {top_n_alleles})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_class_distribution_per_allele.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_length_distribution(df, out_prefix, top_n_alleles=15):\n",
    "    df[\"length\"] = df[\"peptide\"].astype(str).str.len()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.histplot(df[\"length\"], bins=20, kde=True, color=\"steelblue\")\n",
    "    plt.title(\"General Peptide Length Distribution\")\n",
    "    plt.xlabel(\"Peptide length\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_length_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    top_alleles = df[\"allele\"].value_counts().head(top_n_alleles).index\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(\n",
    "        x=\"allele\", y=\"length\",\n",
    "        data=df[df[\"allele\"].isin(top_alleles)],\n",
    "        showfliers=False\n",
    "    )\n",
    "    plt.title(f\"Peptide Length Distribution per Allele (top {top_n_alleles})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_length_distribution_per_allele.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------\n",
    "# Main flow\n",
    "# -----------------------\n",
    "train_parts = []\n",
    "for p in TRAIN_PATHS:\n",
    "    df = load_csv_safe(p)\n",
    "    df[\"peptide\"] = df[\"peptide\"].astype(str).str.upper().str.strip()\n",
    "    df[\"allele\"]  = df[\"allele\"].astype(str).str.strip()\n",
    "    df = clean_allele_column(df, f\"TRAIN file {os.path.basename(p)}\")\n",
    "    train_parts.append(df)\n",
    "train_raw = pd.concat(train_parts, ignore_index=True)\n",
    "\n",
    "test_raw = load_csv_safe(TEST_PATH)\n",
    "test_raw[\"peptide\"] = test_raw[\"peptide\"].astype(str).str.upper().str.strip()\n",
    "test_raw[\"allele\"]  = test_raw[\"allele\"].astype(str).str.strip()\n",
    "test_raw = clean_allele_column(test_raw, \"TEST\")\n",
    "\n",
    "# Stats\n",
    "dataset_stats(train_raw, \"TRAIN (raw)\")\n",
    "dataset_stats(test_raw,  \"TEST (raw)\")\n",
    "\n",
    "# Clean\n",
    "train = remove_exact_duplicates(train_raw, \"TRAIN\")\n",
    "test  = remove_exact_duplicates(test_raw,  \"TEST\")\n",
    "\n",
    "train = remove_conflicting_duplicates(train, \"TRAIN\")\n",
    "if \"hit\" in test.columns:\n",
    "    test = remove_conflicting_duplicates(test, \"TEST\")\n",
    "\n",
    "train = handle_missing(train, \"TRAIN\")\n",
    "test  = handle_missing(test,  \"TEST\")\n",
    "\n",
    "train = clean_invalid_peptides(train, \"TRAIN\")\n",
    "test  = clean_invalid_peptides(test,  \"TEST\")\n",
    "\n",
    "test  = filter_test_alleles_in_train(train, test)\n",
    "\n",
    "# Stats after cleaning\n",
    "dataset_stats(train, \"TRAIN (cleaned)\")\n",
    "dataset_stats(test,  \"TEST (cleaned)\")\n",
    "\n",
    "# Visualization\n",
    "print(\"\\n[INFO] Generating distribution plots...\")\n",
    "plot_class_distribution(train, out_prefix=\"train\")\n",
    "plot_length_distribution(train, out_prefix=\"train\")\n",
    "print(f\"[INFO] Figures saved to {CLEAN_DIR}\")\n",
    "\n",
    "# Compute max length\n",
    "max_seq_len = compute_max_len(train)\n",
    "\n",
    "# Save cleaned data & metadata\n",
    "train_out = os.path.join(CLEAN_DIR, \"train_clean.csv\")\n",
    "test_out  = os.path.join(CLEAN_DIR, \"test_clean.csv\")\n",
    "meta_out  = os.path.join(CLEAN_DIR, \"metadata.json\")\n",
    "\n",
    "train.to_csv(train_out, index=False)\n",
    "test.to_csv(test_out, index=False)\n",
    "\n",
    "metadata = {\n",
    "    \"aa_vocab\": AA_VOCAB,\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"train_alleles\": sorted(train[\"allele\"].unique())\n",
    "}\n",
    "with open(meta_out, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved cleaned TRAIN to: {train_out}\")\n",
    "print(f\"Saved cleaned TEST  to: {test_out}\")\n",
    "print(f\"Saved metadata to: {meta_out}\")\n",
    "print(f\"Saved figures to: {CLEAN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8208e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train rows: 3,679,401\n",
      "Unique alleles in train: 130\n",
      "[Quick] Limiting to top-5 alleles by count.\n",
      "[Quick] After allele limit: 978,016 rows (was 3,679,401) across 5 alleles.\n",
      "[Quick] Allele=HLA-A*02:01          kept  2000 / 265252\n",
      "[Quick] Allele=HLA-A*29:02          kept  2000 / 181136\n",
      "[Quick] Allele=HLA-B*07:02          kept  2000 / 201038\n",
      "[Quick] Allele=HLA-B*40:02          kept  2000 / 145817\n",
      "[Quick] Allele=HLA-B*57:01          kept  2000 / 184773\n",
      "[Quick] Total rows after sampling: 10,000\n",
      "\n",
      "Training with rows: 10,000\n",
      "Alleles to train: 5\n",
      "\n",
      "Per-allele max sequence lengths:\n",
      "  HLA-A*02:01          -> 14\n",
      "  HLA-A*29:02          -> 14\n",
      "  HLA-B*07:02          -> 14\n",
      "  HLA-B*40:02          -> 14\n",
      "  HLA-B*57:01          -> 14\n",
      "... total 5 alleles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yavuz\\Downloads\\instadeep-build\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained allele=HLA-A*02:01          | n_train= 2000 | max_len=14 | saved -> .\\models\\model_HLA-A02_01.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yavuz\\Downloads\\instadeep-build\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained allele=HLA-A*29:02          | n_train= 2000 | max_len=14 | saved -> .\\models\\model_HLA-A29_02.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yavuz\\Downloads\\instadeep-build\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained allele=HLA-B*07:02          | n_train= 2000 | max_len=14 | saved -> .\\models\\model_HLA-B07_02.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yavuz\\Downloads\\instadeep-build\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained allele=HLA-B*40:02          | n_train= 2000 | max_len=14 | saved -> .\\models\\model_HLA-B40_02.joblib\n",
      "Trained allele=HLA-B*57:01          | n_train= 2000 | max_len=14 | saved -> .\\models\\model_HLA-B57_01.joblib\n",
      "\n",
      "Saved model manifest to: .\\models\\manifest.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yavuz\\Downloads\\instadeep-build\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Notebook 2 — Training (train small part of data to check if it works)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import joblib\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_DIR  = \".\"\n",
    "CLEAN_DIR = os.path.join(DATA_DIR, \"clean\")\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"models\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_CLEAN = os.path.join(CLEAN_DIR, \"train_clean.csv\")\n",
    "META_PATH   = os.path.join(CLEAN_DIR, \"metadata.json\")\n",
    "\n",
    "RANDOM_STATE = 53\n",
    "\n",
    "# ===== Quick-run toggles =====\n",
    "QUICK_RUN              = True     # <- set to False to train on FULL data\n",
    "QUICK_ALLELE_LIMIT     = 5        # only train top-N alleles by frequency; None = all\n",
    "QUICK_PER_ALLELE_MAX_N = 2000     # cap samples per allele; None = disable\n",
    "QUICK_FRACTION         = None     # sample fraction per allele (if cap None)\n",
    "# PER_ALLELE_MAX_N overrides FRACTION if both are set.\n",
    "\n",
    "# -----------------------\n",
    "# Load data & metadata\n",
    "# -----------------------\n",
    "train = pd.read_csv(TRAIN_CLEAN)\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "AA_VOCAB    = meta[\"aa_vocab\"]\n",
    "AA_TO_IDX   = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
    "\n",
    "print(f\"Loaded train rows: {len(train):,}\")\n",
    "print(f\"Unique alleles in train: {train['allele'].nunique()}\")\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "\n",
    "def stratified_sample(df, label_col, n_max=None, frac=None, random_state=RANDOM_STATE):\n",
    "    \"\"\"Stratified sampling by label if possible.\"\"\"\n",
    "    if (n_max is None) and (frac is None):\n",
    "        return df\n",
    "    if label_col not in df.columns or df[label_col].nunique() < 2:\n",
    "        if n_max is not None:\n",
    "            n_take = min(len(df), int(n_max))\n",
    "            return df.sample(n=n_take, random_state=random_state) if len(df) > n_take else df\n",
    "        if frac is not None:\n",
    "            return df.sample(frac=frac, random_state=random_state) if 0 < frac < 1.0 else df\n",
    "        return df\n",
    "    parts = []\n",
    "    if n_max is not None:\n",
    "        total = len(df)\n",
    "        for y, dfg in df.groupby(label_col):\n",
    "            k = int(round(len(dfg) / total * n_max))\n",
    "            k = max(1, min(k, len(dfg)))\n",
    "            parts.append(dfg.sample(n=k, random_state=random_state) if len(dfg) > k else dfg)\n",
    "        out = pd.concat(parts, ignore_index=True)\n",
    "        if len(out) < n_max and len(df) > len(out):\n",
    "            remaining = df.drop(out.index, errors=\"ignore\")\n",
    "            need = n_max - len(out)\n",
    "            add = remaining.sample(n=min(need, len(remaining)), random_state=random_state)\n",
    "            out = pd.concat([out, add], ignore_index=True)\n",
    "        return out\n",
    "    if frac is not None and 0 < frac < 1.0:\n",
    "        for y, dfg in df.groupby(label_col):\n",
    "            parts.append(dfg.sample(frac=frac, random_state=random_state))\n",
    "        return pd.concat(parts, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def choose_alleles_for_quick_run(train_df, allele_col=\"allele\", limit=None):\n",
    "    if (limit is None) or (limit <= 0):\n",
    "        return sorted(train_df[allele_col].unique())\n",
    "    counts = train_df[allele_col].value_counts()\n",
    "    chosen = counts.head(limit).index.tolist()\n",
    "    print(f\"[Quick] Limiting to top-{limit} alleles by count.\")\n",
    "    return sorted(chosen)\n",
    "\n",
    "# -----------------------\n",
    "# Optional quick-run filtering\n",
    "# -----------------------\n",
    "if QUICK_RUN:\n",
    "    chosen_alleles = choose_alleles_for_quick_run(train, \"allele\", QUICK_ALLELE_LIMIT)\n",
    "    before = len(train)\n",
    "    train = train[train[\"allele\"].isin(chosen_alleles)].copy()\n",
    "    print(f\"[Quick] After allele limit: {len(train):,} rows (was {before:,}) across {len(chosen_alleles)} alleles.\")\n",
    "\n",
    "    if (QUICK_PER_ALLELE_MAX_N is not None) or (QUICK_FRACTION is not None):\n",
    "        sampled_parts = []\n",
    "        for a in chosen_alleles:\n",
    "            dfa = train[train[\"allele\"] == a]\n",
    "            dfa_s = stratified_sample(\n",
    "                dfa, label_col=\"hit\",\n",
    "                n_max=QUICK_PER_ALLELE_MAX_N if QUICK_PER_ALLELE_MAX_N is not None else None,\n",
    "                frac=QUICK_FRACTION if QUICK_PER_ALLELE_MAX_N is None else None,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "            sampled_parts.append(dfa_s)\n",
    "            print(f\"[Quick] Allele={a:20s} kept {len(dfa_s):5d} / {len(dfa):5d}\")\n",
    "        train = pd.concat(sampled_parts, ignore_index=True)\n",
    "        print(f\"[Quick] Total rows after sampling: {len(train):,}\")\n",
    "\n",
    "print(f\"\\nTraining with rows: {len(train):,}\")\n",
    "print(f\"Alleles to train: {train['allele'].nunique()}\")\n",
    "\n",
    "# -----------------------\n",
    "# Encoder (allele-specific max length)\n",
    "# -----------------------\n",
    "def one_hot_encode_padded(seqs, max_len, aa_to_idx):\n",
    "    \"\"\"One-hot encode peptides up to given max_len.\"\"\"\n",
    "    n = len(seqs)\n",
    "    width = len(aa_to_idx)\n",
    "    X = np.zeros((n, max_len * width), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        s = str(s).strip().upper()\n",
    "        L = min(len(s), max_len)\n",
    "        for j in range(L):\n",
    "            idx = aa_to_idx.get(s[j])\n",
    "            if idx is not None:\n",
    "                X[i, j*width + idx] = 1.0\n",
    "    return X\n",
    "\n",
    "# Compute per-allele max sequence length\n",
    "allele_maxlen = (\n",
    "    train.assign(length=train[\"peptide\"].astype(str).str.len())\n",
    "    .groupby(\"allele\")[\"length\"]\n",
    "    .max()\n",
    "    .astype(int)\n",
    "    .to_dict()\n",
    ")\n",
    "print(\"\\nPer-allele max sequence lengths:\")\n",
    "for allele, L in list(allele_maxlen.items())[:10]:\n",
    "    print(f\"  {allele:20s} -> {L}\")\n",
    "print(f\"... total {len(allele_maxlen)} alleles\")\n",
    "\n",
    "# -----------------------\n",
    "# Train per allele\n",
    "# -----------------------\n",
    "manifest = {\n",
    "    \"aa_vocab\": AA_VOCAB,\n",
    "    \"allele_max_len\": allele_maxlen,\n",
    "    \"models\": []\n",
    "}\n",
    "\n",
    "allele_list = sorted(train[\"allele\"].unique())\n",
    "for allele in allele_list:\n",
    "    tr_full = train[train[\"allele\"] == allele]\n",
    "    if len(tr_full) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = allele_maxlen[allele]\n",
    "    X = one_hot_encode_padded(tr_full[\"peptide\"].tolist(), max_len, AA_TO_IDX)\n",
    "    y = tr_full[\"hit\"].astype(int).values\n",
    "\n",
    "    if len(np.unique(y)) < 2:\n",
    "        clf = DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        clf = LogisticRegression(\n",
    "            penalty=\"l2\",\n",
    "            solver=\"saga\",\n",
    "            max_iter=5000,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    safe_allele = allele.replace(\"*\",\"\").replace(\":\",\"_\").replace(\"/\",\"-\")\n",
    "    model_path = os.path.join(MODEL_DIR, f\"model_{safe_allele}.joblib\")\n",
    "    joblib.dump(clf, model_path)\n",
    "\n",
    "    manifest[\"models\"].append({\n",
    "        \"allele\": allele,\n",
    "        \"path\": model_path,\n",
    "        \"n_train\": int(len(tr_full)),\n",
    "        \"max_len\": int(max_len),\n",
    "        \"model_type\": type(clf).__name__,\n",
    "        \"quick_run\": bool(QUICK_RUN),\n",
    "        \"quick_allele_limit\": int(QUICK_ALLELE_LIMIT) if QUICK_ALLELE_LIMIT is not None else None,\n",
    "        \"quick_per_allele_max_n\": int(QUICK_PER_ALLELE_MAX_N) if QUICK_PER_ALLELE_MAX_N is not None else None,\n",
    "        \"quick_fraction\": float(QUICK_FRACTION) if QUICK_FRACTION is not None else None\n",
    "    })\n",
    "    print(f\"Trained allele={allele:20s} | n_train={len(tr_full):5d} | max_len={max_len:2d} | saved -> {model_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# Save manifest\n",
    "# -----------------------\n",
    "manifest_path = os.path.join(MODEL_DIR, \"manifest.json\")\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved model manifest to: {manifest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bfe751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test rows: 730,233\n",
      "Alleles in test: 32\n",
      "Models available for: 5 alleles\n",
      "Skipping allele=HLA-A*02:05: no trained model found.\n",
      "Skipping allele=HLA-A*02:06: no trained model found.\n",
      "Skipping allele=HLA-A*11:01: no trained model found.\n",
      "Skipping allele=HLA-A*23:01: no trained model found.\n",
      "Skipping allele=HLA-A*25:01: no trained model found.\n",
      "Skipping allele=HLA-A*26:01: no trained model found.\n",
      "Skipping allele=HLA-A*30:01: no trained model found.\n",
      "Skipping allele=HLA-A*30:02: no trained model found.\n",
      "Skipping allele=HLA-A*32:01: no trained model found.\n",
      "Skipping allele=HLA-A*66:01: no trained model found.\n",
      "Skipping allele=HLA-A*68:01: no trained model found.\n",
      "Saved per-allele predictions to: .\\outputs\\predictions\\predictions_HLA-B07_02.csv\n",
      "Evaluated allele=HLA-B*07:02          | n_test= 2469 | max_len=14\n",
      "Skipping allele=HLA-B*08:01: no trained model found.\n",
      "Skipping allele=HLA-B*14:02: no trained model found.\n",
      "Skipping allele=HLA-B*15:01: no trained model found.\n",
      "Skipping allele=HLA-B*15:02: no trained model found.\n",
      "Skipping allele=HLA-B*15:03: no trained model found.\n",
      "Skipping allele=HLA-B*15:17: no trained model found.\n",
      "Skipping allele=HLA-B*18:01: no trained model found.\n",
      "Skipping allele=HLA-B*35:03: no trained model found.\n",
      "Skipping allele=HLA-B*37:01: no trained model found.\n",
      "Skipping allele=HLA-B*38:01: no trained model found.\n",
      "Skipping allele=HLA-B*40:01: no trained model found.\n",
      "Saved per-allele predictions to: .\\outputs\\predictions\\predictions_HLA-B40_02.csv\n",
      "Evaluated allele=HLA-B*40:02          | n_test=23766 | max_len=14\n",
      "Skipping allele=HLA-B*45:01: no trained model found.\n",
      "Skipping allele=HLA-B*46:01: no trained model found.\n",
      "Skipping allele=HLA-B*58:01: no trained model found.\n",
      "Skipping allele=HLA-C*03:03: no trained model found.\n",
      "Skipping allele=HLA-C*05:01: no trained model found.\n",
      "Skipping allele=HLA-C*07:02: no trained model found.\n",
      "Skipping allele=HLA-C*08:02: no trained model found.\n",
      "Skipping allele=HLA-C*12:03: no trained model found.\n",
      "\n",
      "Saved predictions to: .\\outputs\\predictions_per_allele.csv\n",
      "Saved per-allele metrics to: .\\outputs\\metrics_per_allele.csv\n",
      "Saved overall metrics to: .\\outputs\\metrics_overall.json\n",
      "\n",
      "Overall metrics: {'auc': 0.9299817404418763, 'acc': 0.9510196302649133, 'f1': 0.640760413754543}\n"
     ]
    }
   ],
   "source": [
    "# Notebook 3 — Evaluation & Inference\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_DIR   = \".\"\n",
    "CLEAN_DIR  = os.path.join(DATA_DIR, \"clean\")\n",
    "MODEL_DIR  = os.path.join(DATA_DIR, \"models\")\n",
    "OUT_DIR    = os.path.join(DATA_DIR, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TEST_CLEAN = os.path.join(CLEAN_DIR, \"test_clean.csv\")\n",
    "META_PATH  = os.path.join(CLEAN_DIR, \"metadata.json\")\n",
    "MANIFEST   = os.path.join(MODEL_DIR, \"manifest.json\")\n",
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "test = pd.read_csv(TEST_CLEAN)\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "with open(MANIFEST, \"r\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "AA_VOCAB  = meta[\"aa_vocab\"]\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
    "\n",
    "# Per-allele max lengths stored in manifest\n",
    "allele_maxlen = manifest.get(\"allele_max_len\", {})\n",
    "allele_to_model = {m[\"allele\"]: m[\"path\"] for m in manifest[\"models\"]}\n",
    "\n",
    "print(f\"Loaded test rows: {len(test):,}\")\n",
    "print(f\"Alleles in test: {test['allele'].nunique()}\")\n",
    "print(f\"Models available for: {len(allele_to_model)} alleles\")\n",
    "\n",
    "# -----------------------\n",
    "# Encoder\n",
    "# -----------------------\n",
    "def one_hot_encode_padded(seqs, max_len, aa_to_idx):\n",
    "    \"\"\"One-hot encode peptides with padding up to max_len.\"\"\"\n",
    "    n = len(seqs)\n",
    "    width = len(aa_to_idx)\n",
    "    X = np.zeros((n, max_len * width), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        s = str(s).strip().upper()\n",
    "        L = min(len(s), max_len)\n",
    "        for j in range(L):\n",
    "            aa = s[j]\n",
    "            idx = aa_to_idx.get(aa)\n",
    "            if idx is not None:\n",
    "                X[i, j * width + idx] = 1.0\n",
    "    return X\n",
    "\n",
    "# -----------------------\n",
    "# Evaluate per allele\n",
    "# -----------------------\n",
    "pred_rows = []\n",
    "metric_rows = []\n",
    "\n",
    "have_labels = \"hit\" in test.columns and not test[\"hit\"].isna().any()\n",
    "y_true_all, y_pred_all, y_prob_all = [], [], []\n",
    "\n",
    "alleles_in_test = sorted(test[\"allele\"].unique())\n",
    "\n",
    "# Create subdirectory for per-allele outputs\n",
    "ALLELE_OUT_DIR = os.path.join(OUT_DIR, \"predictions\")\n",
    "os.makedirs(ALLELE_OUT_DIR, exist_ok=True)\n",
    "\n",
    "for allele in alleles_in_test:\n",
    "    if allele not in allele_to_model:\n",
    "        print(f\"Skipping allele={allele}: no trained model found.\")\n",
    "        continue\n",
    "\n",
    "    te = test[test[\"allele\"] == allele]\n",
    "    if len(te) == 0:\n",
    "        continue\n",
    "\n",
    "    # Determine correct max_len for this allele\n",
    "    max_len = allele_maxlen.get(allele, meta.get(\"max_seq_len\", 15))\n",
    "\n",
    "    # Encode test peptides\n",
    "    X_te = one_hot_encode_padded(te[\"peptide\"].tolist(), max_len, AA_TO_IDX)\n",
    "    model_path = allele_to_model[allele]\n",
    "    clf = joblib.load(model_path)\n",
    "\n",
    "    # Verify feature size\n",
    "    expected_features = getattr(clf, \"n_features_in_\", X_te.shape[1])\n",
    "    if X_te.shape[1] != expected_features:\n",
    "        print(f\"[WARN] Skipping allele={allele}: feature size mismatch \"\n",
    "              f\"(model={expected_features}, test={X_te.shape[1]})\")\n",
    "        continue\n",
    "\n",
    "    # Predict probabilities\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob = clf.predict_proba(X_te)[:, 1]\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        df = clf.decision_function(X_te)\n",
    "        df_min, df_max = df.min(), df.max()\n",
    "        prob = (df - df_min) / (df_max - df_min + 1e-12)\n",
    "    else:\n",
    "        prob = clf.predict(X_te).astype(float)\n",
    "\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "    te_out = te[[\"peptide\", \"allele\"]].copy()\n",
    "    te_out[\"y_prob\"] = prob\n",
    "    te_out[\"y_pred\"] = pred\n",
    "    if have_labels:\n",
    "        te_out[\"hit\"] = te[\"hit\"].values\n",
    "    pred_rows.append(te_out)\n",
    "\n",
    "    # === NEW: Save per-allele CSV ===\n",
    "    safe_allele = allele.replace('*', '').replace(':', '_').replace('/', '-')\n",
    "    allele_csv = os.path.join(ALLELE_OUT_DIR, f\"predictions_{safe_allele}.csv\")\n",
    "    te_out.to_csv(allele_csv, index=False)\n",
    "    print(f\"Saved per-allele predictions to: {allele_csv}\")\n",
    "\n",
    "    # === Compute per-allele metrics ===\n",
    "    if have_labels:\n",
    "        y_te = te[\"hit\"].astype(int).values\n",
    "        if len(np.unique(y_te)) > 1:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_te, prob)\n",
    "            except ValueError:\n",
    "                auc = np.nan\n",
    "            acc = accuracy_score(y_te, pred)\n",
    "            f1  = f1_score(y_te, pred, zero_division=0)\n",
    "            metric_rows.append({\n",
    "                \"allele\": allele, \"n_test\": len(te),\n",
    "                \"acc\": acc, \"auc\": auc, \"f1\": f1\n",
    "            })\n",
    "            y_true_all.extend(y_te.tolist())\n",
    "            y_pred_all.extend(pred.tolist())\n",
    "            y_prob_all.extend(prob.tolist())\n",
    "        else:\n",
    "            metric_rows.append({\n",
    "                \"allele\": allele, \"n_test\": len(te),\n",
    "                \"acc\": None, \"auc\": None, \"f1\": None\n",
    "            })\n",
    "\n",
    "    print(f\"Evaluated allele={allele:20s} | n_test={len(te):5d} | max_len={max_len}\")\n",
    "\n",
    "# -----------------------\n",
    "# Combine & save\n",
    "# -----------------------\n",
    "if pred_rows:\n",
    "    predictions = pd.concat(pred_rows, ignore_index=True)\n",
    "else:\n",
    "    predictions = pd.DataFrame(columns=[\"peptide\",\"allele\",\"y_prob\",\"y_pred\"] + ([\"hit\"] if have_labels else []))\n",
    "\n",
    "# Overall metrics\n",
    "overall = {}\n",
    "if have_labels and len(y_true_all) > 0 and len(set(y_true_all)) > 1:\n",
    "    try:\n",
    "        overall[\"auc\"] = roc_auc_score(y_true_all, y_prob_all)\n",
    "    except ValueError:\n",
    "        overall[\"auc\"] = np.nan\n",
    "    overall[\"acc\"] = accuracy_score(y_true_all, y_pred_all)\n",
    "    overall[\"f1\"]  = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "else:\n",
    "    overall = {\"auc\": None, \"acc\": None, \"f1\": None}\n",
    "\n",
    "# Save\n",
    "pred_path = os.path.join(OUT_DIR, \"predictions_per_allele.csv\")\n",
    "metrics_path = os.path.join(OUT_DIR, \"metrics_per_allele.csv\")\n",
    "overall_path = os.path.join(OUT_DIR, \"metrics_overall.json\")\n",
    "\n",
    "predictions.to_csv(pred_path, index=False)\n",
    "pd.DataFrame(metric_rows).to_csv(metrics_path, index=False)\n",
    "with open(overall_path, \"w\") as f:\n",
    "    json.dump(overall, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved predictions to: {pred_path}\")\n",
    "print(f\"Saved per-allele metrics to: {metrics_path}\")\n",
    "print(f\"Saved overall metrics to: {overall_path}\")\n",
    "print(\"\\nOverall metrics:\", overall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
