{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_DIR = \"datasets\"\n",
    "TRAIN_FOLDS = [f\"fold_{i}.csv\" for i in range(5)]\n",
    "TEST_FILE = \"test.csv\"\n",
    "\n",
    "TRAIN_PATHS = [os.path.join(DATA_DIR, f) for f in TRAIN_FOLDS]\n",
    "TEST_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "\n",
    "# Output directory\n",
    "CLEAN_DIR = \"clean\"\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "# Valid amino acids (standard 20)\n",
    "AA_VOCAB = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_SET = set(AA_VOCAB)\n",
    "\n",
    "# -----------------------\n",
    "# Allele name correction utilities\n",
    "# -----------------------\n",
    "def fix_allele_format(allele: str) -> str:\n",
    "    \"\"\"Fix allele name to follow standard HLA format: HLA-A*02:101\"\"\"\n",
    "    allele = str(allele).strip().upper()\n",
    "    valid_pattern = re.compile(r\"^HLA-[A-Z]\\*\\d{2}:\\d{2,3}$\")\n",
    "\n",
    "    if valid_pattern.match(allele):\n",
    "        return allele  # already valid\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    allele = allele.replace(\" \", \"\").replace(\"_\", \"\").replace(\"--\", \"-\")\n",
    "\n",
    "    # Ensure starts with HLA-\n",
    "    if not allele.startswith(\"HLA\"):\n",
    "        allele = \"HLA-\" + allele\n",
    "    elif not allele.startswith(\"HLA-\"):\n",
    "        allele = allele.replace(\"HLA\", \"HLA-\", 1)\n",
    "\n",
    "    # Insert \"*\" if missing\n",
    "    if \"*\" not in allele:\n",
    "        allele = re.sub(r\"^(HLA-[A-Z])(\\d+)\", r\"\\1*\\2\", allele)\n",
    "\n",
    "    # Replace wrong separators (e.g., HLA-A*0201 -> HLA-A*02:01)\n",
    "    allele = re.sub(r\"(\\*\\d{2})(\\d{2,3})$\", r\"\\1:\\2\", allele)\n",
    "\n",
    "    return allele\n",
    "\n",
    "\n",
    "def clean_allele_column(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Validate and fix allele names in a DataFrame\"\"\"\n",
    "    print(f\"[{name}] Checking and fixing allele formats...\")\n",
    "\n",
    "    valid_pattern = re.compile(r\"^HLA-[A-Z]\\*\\d{2}:\\d{2,3}$\")\n",
    "\n",
    "    corrections_made = False\n",
    "    corrected_alleles = []\n",
    "\n",
    "    for allele in df[\"allele\"]:\n",
    "        corrected = fix_allele_format(allele)\n",
    "        if corrected != allele:\n",
    "            corrections_made = True\n",
    "        corrected_alleles.append(corrected)\n",
    "\n",
    "    if corrections_made:\n",
    "        print(f\"[DEBUG] Correcting allele naming...\")\n",
    "\n",
    "    df[\"allele\"] = corrected_alleles\n",
    "\n",
    "    invalid_mask = ~df[\"allele\"].str.match(valid_pattern)\n",
    "    invalid_count = invalid_mask.sum()\n",
    "\n",
    "    if invalid_count > 0:\n",
    "        print(f\"[{name}] WARNING: {invalid_count} allele entries still invalid after correction.\")\n",
    "    elif corrections_made:\n",
    "        print(f\"[{name}] All allele names corrected successfully.\")\n",
    "    else:\n",
    "        print(f\"[{name}] No mismatch in allele found.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Utils\n",
    "# -----------------------\n",
    "def load_csv_safe(path: str) -> pd.DataFrame:\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    expected_cols = {\"peptide\", \"allele\", \"hit\"}\n",
    "    missing = expected_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path} missing columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "def dataset_stats(df: pd.DataFrame, name: str):\n",
    "    print(f\"\\n=== Dataset Stats: {name} ===\")\n",
    "    print(f\"Rows: {len(df):,}\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Unique alleles:\", df[\"allele\"].nunique())\n",
    "    if \"hit\" in df.columns:\n",
    "        print(\"Label distribution (hit):\")\n",
    "        print(df[\"hit\"].value_counts(dropna=False))\n",
    "    lens = df[\"peptide\"].astype(str).str.len()\n",
    "    print(\"Peptide length stats:\")\n",
    "    print(lens.describe())\n",
    "\n",
    "def remove_exact_duplicates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    before = len(df)\n",
    "    df2 = df.drop_duplicates()\n",
    "    print(f\"[{name}] Removed exact duplicate rows: {before - len(df2)}\")\n",
    "    return df2\n",
    "\n",
    "def remove_conflicting_duplicates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    grp = df.groupby([\"peptide\", \"allele\"])[\"hit\"].nunique()\n",
    "    conflicts = grp[grp > 1]\n",
    "    if conflicts.empty:\n",
    "        print(f\"[{name}] Conflicting duplicates: none.\")\n",
    "        return df\n",
    "    before = len(df)\n",
    "    bad_keys = set(conflicts.index)\n",
    "    mask = df.set_index([\"peptide\", \"allele\"]).index.isin(bad_keys)\n",
    "    df2 = df[~mask].copy()\n",
    "    print(f\"[{name}] Conflicting (peptide, allele) pairs: {len(conflicts)} | Rows removed: {before - len(df2)}\")\n",
    "    return df2\n",
    "\n",
    "def handle_missing(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    miss = df.isna().sum()\n",
    "    if miss.sum() == 0:\n",
    "        print(f\"[{name}] Missing values: none.\")\n",
    "        return df\n",
    "    print(f\"[{name}] Missing values per column:\\n{miss}\")\n",
    "    before = len(df)\n",
    "    df2 = df.dropna(subset=[\"peptide\", \"allele\", \"hit\"]).copy()\n",
    "    print(f\"[{name}] Rows dropped due to missing peptide/allele/hit: {before - len(df2)}\")\n",
    "    return df2\n",
    "\n",
    "def is_valid_peptide(seq: str) -> bool:\n",
    "    s = str(seq).strip().upper()\n",
    "    if len(s) == 0:\n",
    "        return False\n",
    "    return set(s).issubset(AA_SET)\n",
    "\n",
    "def clean_invalid_peptides(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    lens_before = df[\"peptide\"].astype(str).str.len().describe()\n",
    "    valid_mask = df[\"peptide\"].astype(str).str.upper().apply(is_valid_peptide)\n",
    "    invalid = (~valid_mask).sum()\n",
    "    df2 = df[valid_mask].copy()\n",
    "    print(f\"[{name}] Non-standard/invalid peptide rows removed: {invalid}\")\n",
    "    print(f\"[{name}] Sequence length stats BEFORE removal:\\n{lens_before}\")\n",
    "    print(f\"[{name}] Sequence length stats AFTER removal:\\n{df2['peptide'].astype(str).str.len().describe()}\")\n",
    "    return df2\n",
    "\n",
    "def filter_test_alleles_in_train(train: pd.DataFrame, test: pd.DataFrame) -> pd.DataFrame:\n",
    "    train_alleles = set(train[\"allele\"].unique())\n",
    "    test_alleles  = set(test[\"allele\"].unique())\n",
    "    unknown = sorted(list(test_alleles - train_alleles))\n",
    "    if unknown:\n",
    "        before = len(test)\n",
    "        test2 = test[test[\"allele\"].isin(train_alleles)].copy()\n",
    "        print(f\"[TEST] Alleles not present in TRAIN: {unknown}\")\n",
    "        print(f\"[TEST] Removed rows with unknown alleles: {before - len(test2)}\")\n",
    "        return test2\n",
    "    print(\"[TEST] All test alleles exist in training.\")\n",
    "    return test\n",
    "\n",
    "def compute_max_len(train_df: pd.DataFrame) -> int:\n",
    "    max_len = int(train_df[\"peptide\"].astype(str).str.len().max())\n",
    "    print(f\"[FE] Max sequence length (from CLEANED TRAIN): {max_len}\")\n",
    "    return max_len\n",
    "\n",
    "# -----------------------\n",
    "# Visualization helpers\n",
    "# -----------------------\n",
    "def plot_class_distribution(df, out_prefix, top_n_alleles=15):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.countplot(x=\"hit\", data=df)\n",
    "    plt.title(\"General Class Distribution (hit)\")\n",
    "    plt.xlabel(\"Class (hit)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_class_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    allele_counts = (\n",
    "        df.groupby([\"allele\",\"hit\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    top_alleles = df[\"allele\"].value_counts().head(top_n_alleles).index\n",
    "    sns.barplot(\n",
    "        x=\"allele\", y=\"count\", hue=\"hit\",\n",
    "        data=allele_counts[allele_counts[\"allele\"].isin(top_alleles)]\n",
    "    )\n",
    "    plt.title(f\"Class Distribution per Allele (top {top_n_alleles})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_class_distribution_per_allele.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_length_distribution(df, out_prefix, top_n_alleles=15):\n",
    "    df[\"length\"] = df[\"peptide\"].astype(str).str.len()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.histplot(df[\"length\"], bins=20, kde=True, color=\"steelblue\")\n",
    "    plt.title(\"General Peptide Length Distribution\")\n",
    "    plt.xlabel(\"Peptide length\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_length_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    top_alleles = df[\"allele\"].value_counts().head(top_n_alleles).index\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(\n",
    "        x=\"allele\", y=\"length\",\n",
    "        data=df[df[\"allele\"].isin(top_alleles)],\n",
    "        showfliers=False\n",
    "    )\n",
    "    plt.title(f\"Peptide Length Distribution per Allele (top {top_n_alleles})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CLEAN_DIR, f\"{out_prefix}_length_distribution_per_allele.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------\n",
    "# Main flow\n",
    "# -----------------------\n",
    "train_parts = []\n",
    "for p in TRAIN_PATHS:\n",
    "    df = load_csv_safe(p)\n",
    "    df[\"peptide\"] = df[\"peptide\"].astype(str).str.upper().str.strip()\n",
    "    df[\"allele\"]  = df[\"allele\"].astype(str).str.strip()\n",
    "    df = clean_allele_column(df, f\"TRAIN file {os.path.basename(p)}\")\n",
    "    train_parts.append(df)\n",
    "train_raw = pd.concat(train_parts, ignore_index=True)\n",
    "\n",
    "test_raw = load_csv_safe(TEST_PATH)\n",
    "test_raw[\"peptide\"] = test_raw[\"peptide\"].astype(str).str.upper().str.strip()\n",
    "test_raw[\"allele\"]  = test_raw[\"allele\"].astype(str).str.strip()\n",
    "test_raw = clean_allele_column(test_raw, \"TEST\")\n",
    "\n",
    "# Stats\n",
    "dataset_stats(train_raw, \"TRAIN (raw)\")\n",
    "dataset_stats(test_raw,  \"TEST (raw)\")\n",
    "\n",
    "# Clean\n",
    "train = remove_exact_duplicates(train_raw, \"TRAIN\")\n",
    "test  = remove_exact_duplicates(test_raw,  \"TEST\")\n",
    "\n",
    "train = remove_conflicting_duplicates(train, \"TRAIN\")\n",
    "if \"hit\" in test.columns:\n",
    "    test = remove_conflicting_duplicates(test, \"TEST\")\n",
    "\n",
    "train = handle_missing(train, \"TRAIN\")\n",
    "test  = handle_missing(test,  \"TEST\")\n",
    "\n",
    "train = clean_invalid_peptides(train, \"TRAIN\")\n",
    "test  = clean_invalid_peptides(test,  \"TEST\")\n",
    "\n",
    "test  = filter_test_alleles_in_train(train, test)\n",
    "\n",
    "# Stats after cleaning\n",
    "dataset_stats(train, \"TRAIN (cleaned)\")\n",
    "dataset_stats(test,  \"TEST (cleaned)\")\n",
    "\n",
    "# Visualization\n",
    "print(\"\\n[INFO] Generating distribution plots...\")\n",
    "plot_class_distribution(train, out_prefix=\"train\")\n",
    "plot_length_distribution(train, out_prefix=\"train\")\n",
    "print(f\"[INFO] Figures saved to {CLEAN_DIR}\")\n",
    "\n",
    "# Compute max length\n",
    "max_seq_len = compute_max_len(train)\n",
    "\n",
    "# Save cleaned data & metadata\n",
    "train_out = os.path.join(CLEAN_DIR, \"train_clean.csv\")\n",
    "test_out  = os.path.join(CLEAN_DIR, \"test_clean.csv\")\n",
    "meta_out  = os.path.join(CLEAN_DIR, \"metadata.json\")\n",
    "\n",
    "train.to_csv(train_out, index=False)\n",
    "test.to_csv(test_out, index=False)\n",
    "\n",
    "metadata = {\n",
    "    \"aa_vocab\": AA_VOCAB,\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"train_alleles\": sorted(train[\"allele\"].unique())\n",
    "}\n",
    "with open(meta_out, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved cleaned TRAIN to: {train_out}\")\n",
    "print(f\"Saved cleaned TEST  to: {test_out}\")\n",
    "print(f\"Saved metadata to: {meta_out}\")\n",
    "print(f\"Saved figures to: {CLEAN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 2 — Training (Full Dataset with Batch Processing)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_DIR  = \".\"\n",
    "CLEAN_DIR = os.path.join(DATA_DIR, \"clean\")\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"models\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_CLEAN = os.path.join(CLEAN_DIR, \"train_clean.csv\")\n",
    "META_PATH   = os.path.join(CLEAN_DIR, \"metadata.json\")\n",
    "\n",
    "RANDOM_STATE = 53\n",
    "CHUNK_SIZE   = 50000       # number of rows processed per batch per allele\n",
    "EPOCHS       = 10           # 🔁 number of passes over the full data\n",
    "MAX_ITER     = 5            # internal iteration for partial_fit\n",
    "\n",
    "# -----------------------\n",
    "# Load data & metadata\n",
    "# -----------------------\n",
    "print(\"[INFO] Loading cleaned training data...\")\n",
    "train = pd.read_csv(TRAIN_CLEAN)\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "AA_VOCAB  = meta[\"aa_vocab\"]\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train):,} rows | {train['allele'].nunique()} alleles\")\n",
    "\n",
    "# -----------------------\n",
    "# Encoder\n",
    "# -----------------------\n",
    "def one_hot_encode_padded(seqs, max_len, aa_to_idx):\n",
    "    \"\"\"One-hot encode peptides up to given max_len.\"\"\"\n",
    "    n = len(seqs)\n",
    "    width = len(aa_to_idx)\n",
    "    X = np.zeros((n, max_len * width), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        s = str(s).strip().upper()\n",
    "        L = min(len(s), max_len)\n",
    "        for j in range(L):\n",
    "            idx = aa_to_idx.get(s[j])\n",
    "            if idx is not None:\n",
    "                X[i, j*width + idx] = 1.0\n",
    "    return X\n",
    "\n",
    "# Compute per-allele max sequence length\n",
    "allele_maxlen = (\n",
    "    train.assign(length=train[\"peptide\"].astype(str).str.len())\n",
    "    .groupby(\"allele\")[\"length\"]\n",
    "    .max()\n",
    "    .astype(int)\n",
    "    .to_dict()\n",
    ")\n",
    "print(f\"[INFO] Computed max sequence lengths for {len(allele_maxlen)} alleles.\")\n",
    "\n",
    "# -----------------------\n",
    "# Training per allele (chunked incremental mode)\n",
    "# -----------------------\n",
    "manifest = {\n",
    "    \"aa_vocab\": AA_VOCAB,\n",
    "    \"allele_max_len\": allele_maxlen,\n",
    "    \"models\": []\n",
    "}\n",
    "\n",
    "allele_list = sorted(train[\"allele\"].unique())\n",
    "\n",
    "for idx, allele in enumerate(allele_list, 1):\n",
    "    tr_full = train[train[\"allele\"] == allele]\n",
    "    if len(tr_full) == 0:\n",
    "        continue\n",
    "\n",
    "    n_samples = len(tr_full)\n",
    "    max_len = allele_maxlen[allele]\n",
    "    print(f\"\\n[{idx}/{len(allele_list)}] Training allele={allele} | n={n_samples:,} | max_len={max_len}\")\n",
    "\n",
    "    y_full = tr_full[\"hit\"].astype(int).values\n",
    "\n",
    "    # --- Handle single-class allele ---\n",
    "    if len(np.unique(y_full)) < 2:\n",
    "        clf = DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "        clf.fit(np.zeros((1, max_len * len(AA_TO_IDX))), [y_full[0]])\n",
    "        print(f\"[INFO] Only one class present → DummyClassifier.\")\n",
    "    else:\n",
    "        # --- Compute manual class weights ---\n",
    "        classes = np.array([0, 1])\n",
    "        weights = compute_class_weight(\"balanced\", classes=classes, y=y_full)\n",
    "        class_weight_dict = {cls: w for cls, w in zip(classes, weights)}\n",
    "        print(f\"[INFO] Computed class weights: {class_weight_dict}\")\n",
    "\n",
    "        # --- Initialize incremental model ---\n",
    "        clf = SGDClassifier(\n",
    "            loss=\"log_loss\",\n",
    "            penalty=\"l2\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_iter=MAX_ITER,\n",
    "            validation_fraction=0.1,\n",
    "            learning_rate=\"optimal\",\n",
    "            tol=1e-3,\n",
    "            n_jobs=-1,          # ✅ use all CPUs\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "        # Shuffle once before training\n",
    "        tr_full = tr_full.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "        # 🔁 Multiple epochs\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f\"  [Epoch {epoch+1}/{EPOCHS}] ----------------------\")\n",
    "\n",
    "            for start in range(0, n_samples, CHUNK_SIZE):\n",
    "                end = min(start + CHUNK_SIZE, n_samples)\n",
    "                batch = tr_full.iloc[start:end]\n",
    "                X_batch = one_hot_encode_padded(batch[\"peptide\"].tolist(), max_len, AA_TO_IDX)\n",
    "                y_batch = batch[\"hit\"].astype(int).values\n",
    "\n",
    "                sample_weight = np.array([class_weight_dict[y] for y in y_batch])\n",
    "\n",
    "                # partial_fit trains incrementally\n",
    "                if epoch == 0 and start == 0:\n",
    "                    clf.fit(X_batch, y_batch, sample_weight=sample_weight)\n",
    "                else:\n",
    "                    clf.fit(X_batch, y_batch, sample_weight=sample_weight)\n",
    "\n",
    "                del X_batch, y_batch, batch, sample_weight\n",
    "                gc.collect()\n",
    "                print(f\"    [Batch {start//CHUNK_SIZE+1}] trained on rows {start:,}-{end:,}\")\n",
    "\n",
    "            # Optional: reshuffle each epoch for better convergence\n",
    "            tr_full = tr_full.sample(frac=1.0, random_state=RANDOM_STATE + epoch).reset_index(drop=True)\n",
    "\n",
    "    # --- Save model ---\n",
    "    safe_allele = allele.replace(\"*\", \"\").replace(\":\", \"_\").replace(\"/\", \"-\")\n",
    "    model_path = os.path.join(MODEL_DIR, f\"model_{safe_allele}.joblib\")\n",
    "    joblib.dump(clf, model_path)\n",
    "\n",
    "    manifest[\"models\"].append({\n",
    "        \"allele\": allele,\n",
    "        \"path\": model_path,\n",
    "        \"n_train\": int(n_samples),\n",
    "        \"max_len\": int(max_len),\n",
    "        \"model_type\": type(clf).__name__,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"epochs\": EPOCHS\n",
    "    })\n",
    "\n",
    "    print(f\"[DONE] Saved model for {allele} -> {model_path}\")\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------\n",
    "# Save manifest\n",
    "# -----------------------\n",
    "manifest_path = os.path.join(MODEL_DIR, \"manifest.json\")\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Saved manifest to {manifest_path}\")\n",
    "print(\"[INFO] All models trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 3 — Evaluation & Inference\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_DIR   = \".\"\n",
    "CLEAN_DIR  = os.path.join(DATA_DIR, \"clean\")\n",
    "MODEL_DIR  = os.path.join(DATA_DIR, \"models\")\n",
    "OUT_DIR    = os.path.join(DATA_DIR, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TEST_CLEAN = os.path.join(CLEAN_DIR, \"test_clean.csv\")\n",
    "META_PATH  = os.path.join(CLEAN_DIR, \"metadata.json\")\n",
    "MANIFEST   = os.path.join(MODEL_DIR, \"manifest.json\")\n",
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "test = pd.read_csv(TEST_CLEAN)\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "with open(MANIFEST, \"r\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "AA_VOCAB  = meta[\"aa_vocab\"]\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
    "\n",
    "# Per-allele max lengths stored in manifest\n",
    "allele_maxlen = manifest.get(\"allele_max_len\", {})\n",
    "allele_to_model = {m[\"allele\"]: m[\"path\"] for m in manifest[\"models\"]}\n",
    "\n",
    "print(f\"Loaded test rows: {len(test):,}\")\n",
    "print(f\"Alleles in test: {test['allele'].nunique()}\")\n",
    "print(f\"Models available for: {len(allele_to_model)} alleles\")\n",
    "\n",
    "# -----------------------\n",
    "# Encoder\n",
    "# -----------------------\n",
    "def one_hot_encode_padded(seqs, max_len, aa_to_idx):\n",
    "    \"\"\"One-hot encode peptides with padding up to max_len.\"\"\"\n",
    "    n = len(seqs)\n",
    "    width = len(aa_to_idx)\n",
    "    X = np.zeros((n, max_len * width), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        s = str(s).strip().upper()\n",
    "        L = min(len(s), max_len)\n",
    "        for j in range(L):\n",
    "            aa = s[j]\n",
    "            idx = aa_to_idx.get(aa)\n",
    "            if idx is not None:\n",
    "                X[i, j * width + idx] = 1.0\n",
    "    return X\n",
    "\n",
    "# -----------------------\n",
    "# Evaluate per allele\n",
    "# -----------------------\n",
    "pred_rows = []\n",
    "metric_rows = []\n",
    "\n",
    "have_labels = \"hit\" in test.columns and not test[\"hit\"].isna().any()\n",
    "y_true_all, y_pred_all, y_prob_all = [], [], []\n",
    "\n",
    "alleles_in_test = sorted(test[\"allele\"].unique())\n",
    "\n",
    "# Create subdirectory for per-allele outputs\n",
    "ALLELE_OUT_DIR = os.path.join(OUT_DIR, \"predictions\")\n",
    "os.makedirs(ALLELE_OUT_DIR, exist_ok=True)\n",
    "\n",
    "for allele in alleles_in_test:\n",
    "    if allele not in allele_to_model:\n",
    "        print(f\"Skipping allele={allele}: no trained model found.\")\n",
    "        continue\n",
    "\n",
    "    te = test[test[\"allele\"] == allele]\n",
    "    if len(te) == 0:\n",
    "        continue\n",
    "\n",
    "    # Determine correct max_len for this allele\n",
    "    max_len = allele_maxlen.get(allele, meta.get(\"max_seq_len\", 15))\n",
    "\n",
    "    # Encode test peptides\n",
    "    X_te = one_hot_encode_padded(te[\"peptide\"].tolist(), max_len, AA_TO_IDX)\n",
    "    model_path = allele_to_model[allele]\n",
    "    clf = joblib.load(model_path)\n",
    "\n",
    "    # Verify feature size\n",
    "    expected_features = getattr(clf, \"n_features_in_\", X_te.shape[1])\n",
    "    if X_te.shape[1] != expected_features:\n",
    "        print(f\"[WARN] Skipping allele={allele}: feature size mismatch \"\n",
    "              f\"(model={expected_features}, test={X_te.shape[1]})\")\n",
    "        continue\n",
    "\n",
    "    # Predict probabilities\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob = clf.predict_proba(X_te)[:, 1]\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        df = clf.decision_function(X_te)\n",
    "        df_min, df_max = df.min(), df.max()\n",
    "        prob = (df - df_min) / (df_max - df_min + 1e-12)\n",
    "    else:\n",
    "        prob = clf.predict(X_te).astype(float)\n",
    "\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "    te_out = te[[\"peptide\", \"allele\"]].copy()\n",
    "    te_out[\"y_prob\"] = prob\n",
    "    te_out[\"y_pred\"] = pred\n",
    "    if have_labels:\n",
    "        te_out[\"hit\"] = te[\"hit\"].values\n",
    "    pred_rows.append(te_out)\n",
    "\n",
    "    # === NEW: Save per-allele CSV ===\n",
    "    safe_allele = allele.replace('*', '').replace(':', '_').replace('/', '-')\n",
    "    allele_csv = os.path.join(ALLELE_OUT_DIR, f\"predictions_{safe_allele}.csv\")\n",
    "    te_out.to_csv(allele_csv, index=False)\n",
    "    print(f\"Saved per-allele predictions to: {allele_csv}\")\n",
    "\n",
    "    # === Compute per-allele metrics ===\n",
    "    if have_labels:\n",
    "        y_te = te[\"hit\"].astype(int).values\n",
    "        if len(np.unique(y_te)) > 1:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_te, prob)\n",
    "            except ValueError:\n",
    "                auc = np.nan\n",
    "            acc = accuracy_score(y_te, pred)\n",
    "            f1  = f1_score(y_te, pred, zero_division=0)\n",
    "            metric_rows.append({\n",
    "                \"allele\": allele, \"n_test\": len(te),\n",
    "                \"acc\": acc, \"auc\": auc, \"f1\": f1\n",
    "            })\n",
    "            y_true_all.extend(y_te.tolist())\n",
    "            y_pred_all.extend(pred.tolist())\n",
    "            y_prob_all.extend(prob.tolist())\n",
    "        else:\n",
    "            metric_rows.append({\n",
    "                \"allele\": allele, \"n_test\": len(te),\n",
    "                \"acc\": None, \"auc\": None, \"f1\": None\n",
    "            })\n",
    "\n",
    "    print(f\"Evaluated allele={allele:20s} | n_test={len(te):5d} | max_len={max_len}\")\n",
    "\n",
    "# -----------------------\n",
    "# Combine & save\n",
    "# -----------------------\n",
    "if pred_rows:\n",
    "    predictions = pd.concat(pred_rows, ignore_index=True)\n",
    "else:\n",
    "    predictions = pd.DataFrame(columns=[\"peptide\",\"allele\",\"y_prob\",\"y_pred\"] + ([\"hit\"] if have_labels else []))\n",
    "\n",
    "# Overall metrics\n",
    "overall = {}\n",
    "if have_labels and len(y_true_all) > 0 and len(set(y_true_all)) > 1:\n",
    "    try:\n",
    "        overall[\"auc\"] = roc_auc_score(y_true_all, y_prob_all)\n",
    "    except ValueError:\n",
    "        overall[\"auc\"] = np.nan\n",
    "    overall[\"acc\"] = accuracy_score(y_true_all, y_pred_all)\n",
    "    overall[\"f1\"]  = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "else:\n",
    "    overall = {\"auc\": None, \"acc\": None, \"f1\": None}\n",
    "\n",
    "# Save\n",
    "pred_path = os.path.join(OUT_DIR, \"predictions_per_allele.csv\")\n",
    "metrics_path = os.path.join(OUT_DIR, \"metrics_per_allele.csv\")\n",
    "overall_path = os.path.join(OUT_DIR, \"metrics_overall.json\")\n",
    "\n",
    "predictions.to_csv(pred_path, index=False)\n",
    "pd.DataFrame(metric_rows).to_csv(metrics_path, index=False)\n",
    "with open(overall_path, \"w\") as f:\n",
    "    json.dump(overall, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved predictions to: {pred_path}\")\n",
    "print(f\"Saved per-allele metrics to: {metrics_path}\")\n",
    "print(f\"Saved overall metrics to: {overall_path}\")\n",
    "print(\"\\nOverall metrics:\", overall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
